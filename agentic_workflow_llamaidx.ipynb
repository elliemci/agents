{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfH0QyM4BfRgW7NCu277Nx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elliemci/agents/blob/main/agentic_workflow_llamaidx.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LlamaIndex Agentic Workflows"
      ],
      "metadata": {
        "id": "2Sp7y4gZ9EDp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Workflows are the building blocks of agent or a multi-agent system"
      ],
      "metadata": {
        "id": "4UP2mtIX9zaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index"
      ],
      "metadata": {
        "id": "E6NmGcR39MWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-llms-huggingface-api"
      ],
      "metadata": {
        "id": "ueMDSxOFE0jF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tavily-python"
      ],
      "metadata": {
        "id": "E0ipIl0_EH44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58UEVCNk9BdV",
        "outputId": "e6144923-7fb5-4e37-9a08-ed9c2407a99e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/ColabNotebooks/AgentsCourse\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/ColabNotebooks/AgentsCourse"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agentic Workflow with One Agent"
      ],
      "metadata": {
        "id": "vfgjdVpr-KXS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLM"
      ],
      "metadata": {
        "id": "RZNdvj6a_mO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
        "\n",
        "hf_hub_token = userdata.get('huggingface_hub_access_token')\n",
        "login(token=hf_hub_token)\n",
        "\n",
        "tavily_api_key = userdata.get('TAVILY_API_KEY')"
      ],
      "metadata": {
        "id": "Tn5whHma_8Z2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiating an LLM\n",
        "llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")"
      ],
      "metadata": {
        "id": "O_X-n5W5AoY4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install llama-index llama-index-llms-openai\n",
        "\n",
        "# from llama_index.llms.OpenAI import OpenAI\n",
        "\n",
        "# openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "# llm = OpenAI(model=\"gpt-4o-mini\", api_key=api_key)"
      ],
      "metadata": {
        "id": "a0xXXqocN-cR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The engine of any agent is the LLM that does the text processing and reasoning"
      ],
      "metadata": {
        "id": "it2KsCse_qs1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tool"
      ],
      "metadata": {
        "id": "Ze_HdqmkKfK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Agents** are not using a traditional programing logic of define steps, but they are semi-autonomous, using a set of tools and having a goal, they will use these tool to find out more about the state of the world, take action in response to instructions and achive the goal. Tavity, a smart search engine, designed to be used by LLMs provides a web search tool to agents.<br><br>\n",
        "**Tools** are regular python functions with\n",
        "* name and description which are used by the LLM to understand what the tool does\n",
        "* tool's metadata, input and output types are anotated for the LLM to \"understand\" how to use the tool\n",
        "* `async` is used to make workflow more efficient"
      ],
      "metadata": {
        "id": "-Bqtcvm0Dn4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tavily import AsyncTavilyClient\n",
        "\n",
        "async def search_web(query: str) -> str:  # input and output type annotations\n",
        "  \"\"\"Useful for using the web to answer questions.\"\"\"\n",
        "  client = AsyncTavilyClient(api_key=tavily_api_key)\n",
        "  return str(await client.search(query))"
      ],
      "metadata": {
        "id": "3aGqwX9mJod5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AgentWorkflow"
      ],
      "metadata": {
        "id": "a1KeyGBJKtHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A system prompt defines what the agent does. Initializes an AgentWorkflow from a list of tools or functions.\n",
        "\n",
        "If the LLM is a function calling model, the workflow will use the FunctionAgent, otherwise, it will use the ReActAgent."
      ],
      "metadata": {
        "id": "tzpcUOzzK7kC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent.workflow import AgentWorkflow\n",
        "\n",
        "workflow = AgentWorkflow.from_tools_or_functions(\n",
        "    [search_web],\n",
        "    llm=llm,\n",
        "    system_prompt=\"You are a helpful assistant that answers questions. If you dont know the answer, you can search the web for information.\"\n",
        ")"
      ],
      "metadata": {
        "id": "UYrtsWjDLOwI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running the Agent"
      ],
      "metadata": {
        "id": "AHA1iPrNMXHT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An AgentWorkflow expext to start with a question or prompt, user_msg, which is passed to the agent"
      ],
      "metadata": {
        "id": "QujzRIfBMdJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = await workflow.run(user_msg=\"What is the weather in Portland, OR?\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_1McXioMrA-",
        "outputId": "0560dce3-7ec0-4df8-eddf-2dcdd909ddcf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The current weather in Portland, OR is partly cloudy with a temperature of 6.7°C (44.1°F). The wind speed is 2.2 mph (3.6 km/h) from the NNE, and the humidity is 76%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### State"
      ],
      "metadata": {
        "id": "cnhovLysOR5Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, AgenticWorkflow is stateless, to keep a memory of previous runs, Workflows need to mantain state within and between runs with a `Context`"
      ],
      "metadata": {
        "id": "WfC5faq5OTzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.workflow import Context\n",
        "\n",
        "# instantiate a new Context passing in the workflow to properly configure the Context object\n",
        "ctx = Context(workflow)\n",
        "\n",
        "response = await workflow.run(\n",
        "    user_msg=\"My name is Ellie, nice to meet you.\",\n",
        "    ctx=ctx  # give the configured context to the workflow\n",
        "    )\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3TPXf4wPB49",
        "outputId": "8bd672ee-7c01-4c26-dce0-8a56f010f1dd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nice to meet you, Ellie! It's great to finally put a face to the name. How are you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pass the same context to a second run\n",
        "response = await workflow.run(\n",
        "    user_msg=\"What is my name?\",\n",
        "    ctx=ctx\n",
        "    )\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2u_ppcSQdmH",
        "outputId": "ff83bd40-80fa-41e1-adee-b88ff348432f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your name is Ellie.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Context"
      ],
      "metadata": {
        "id": "2JMQeYGmRDi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The context is an object of memory and it is serializable and can be saved to a database, file, etc and loaded back later. `JsonSerializer` uses `json.dumps` and `json.loads` to serialize and deserialize the context, while `JsonPickleSerializer` uses `pickle` and can be used for context with not serializable objects"
      ],
      "metadata": {
        "id": "-QdY-5-0RHdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.workflow import JsonPickleSerializer, JsonSerializer\n",
        "\n",
        "# convert Context to dictionary obj\n",
        "ctx_dict = ctx.to_dict(serializer=JsonSerializer())\n",
        "# create a new Context from the dictionary\n",
        "restored_ctx = Context.from_dict(workflow, ctx_dict, serializer=JsonSerializer())\n",
        "\n",
        "response = await workflow.run(\n",
        "    user_msg=\"Do you still remember my name?\",\n",
        "    ctx=restored_ctx\n",
        "    )\n",
        "\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cItdppplSoDj",
        "outputId": "10ce71a8-b1f4-4e8a-a811-9017f63c23b6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, I still remember your name. Your name is Ellie.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Streaming"
      ],
      "metadata": {
        "id": "gOqURt4PUWrI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`AgentWorkflow` can be streamed using a handler, which returns a variety of event types as the workflow executes\n",
        "* `AgentStream` events stream the LLM output\n",
        "* `AgentInput` events return the running agent\n",
        "* `AgentOutput` events returns called tools and agent outputs\n",
        "* `ToolCall` and `ToolCallResults` track tool calls and outputs"
      ],
      "metadata": {
        "id": "Cao_Ol99UbdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent.workflow import AgentInput, AgentOutput, AgentStream, ToolCall, ToolCallResult\n",
        "\n",
        "hangler = workflow.run(user_msg=\"What is the weather in Montreal?\")\n",
        "\n",
        "# streaming the response as each of the deltas are arriving\n",
        "async for event in hangler.stream_events():\n",
        "  if isinstance(event, AgentStream):       # handle AgentStream events\n",
        "    print(event.delta, end=\"\", flush=True) # delta is the latest chunk of text recived from the llm\n",
        "    #print(event.response)  # the current full response\n",
        "    #print(event.raw)       # the raw llm api response\n",
        "    #print(event.current_agent_name)\n",
        " #elif isinstance(event, AgentInput):\n",
        "    #print(\"Agent input:\", event.input) # the current input message\n",
        "    #print(\"Agent name:\", event.current_agent_name)\n",
        " #elif isinstance(event, AgentOutput):\n",
        "    #print(\"Agent output:\", event.response)\n",
        "    #print(\"Tool calls made:\", event.tool_calls)\n",
        "    #print(\"Raw LLM response:\", event.raw)  # the raw llm api response\n",
        " #elif isinstance(event, ToolCall):\n",
        "    #print(\"Tool called: \", event.tool_name)\n",
        "    #print(\"Arguments to the tool:\", event.tool_kwargs)\n",
        "    #print(\"Tool output:\", event.tool_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2topuO1Y39w",
        "outputId": "5462dd7f-d0be-4dcc-9ab6-eadc83be72cc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
            "Action: search_web\n",
            "Action Input: {\"query\": \"current weather in Montreal\"}Thought: I can answer without using any more tools. I'll use the user's language to answer\n",
            "Answer: The current weather in Montreal is overcast with a temperature of 0.2°C (32.4°F). It is currently day, and the wind is blowing at 6.3 mph (10.1 kph) from the WSW direction. The humidity is 74%, and the cloud cover is 100%."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### State Tools"
      ],
      "metadata": {
        "id": "A-Qtw68MegQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tools can be define to have access to the workflow context. The can set and retrive variables from the context. AgentWorkflow uses a context variable `state` that gets passed to every agent."
      ],
      "metadata": {
        "id": "hUTywEucevIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.workflow import Context\n",
        "\n",
        "# for the AgentWOrkflow to automatically pass the Context, the Context parameter should be the first input paramer of the tool\n",
        "async def set_name(ctx: Context, name: str) -> str:\n",
        "  state = await ctx.get(\"state\")\n",
        "  state[\"name\"] = name\n",
        "  await ctx.set(\"state\", state)\n",
        "  return f\"Name set to {name}\"\n",
        "\n",
        "workflow = AgentWorkflow.from_tools_or_functions(\n",
        "    [set_name],\n",
        "    llm=llm,\n",
        "    system_prompt=\"You are a helpful assistant that can set a name.\",\n",
        "    initial_state={\"name\":\"unset\"}\n",
        ")\n",
        "\n",
        "# initialize the context\n",
        "ctx = Context(workflow)\n",
        "\n",
        "# run the workflow\n",
        "response = await workflow.run(user_msg=\"My name is Ellie\", ctx=ctx)\n",
        "print(str(response))\n",
        "\n",
        "# access the state\n",
        "state = await ctx.get(\"state\")\n",
        "print(\"Name as stored in state: \", state[\"name\"])\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUpr35IzZcm_",
        "outputId": "d1e576d4-fb6a-4341-b9da-c84130a99246"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your name has been set to Ellie.\n",
            "Name as stored in state:  Ellie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Human in the Loop"
      ],
      "metadata": {
        "id": "LjaW3n8IPG5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tool emits, `write_event_to_stream`,  an event that isn't recived by any other step in the workflow, the tool weaits till it receives a `InputRequiredEvent` or `HumanResponseEvent` events which can be subclassed to match the needs"
      ],
      "metadata": {
        "id": "AzaZnOq-RQfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.workflow import Context, InputRequiredEvent, HumanResponseEvent\n",
        "from llama_index.core.agent.workflow import AgentWorkflow\n",
        "\n",
        "# a tool that [erforms a dangerous task\n",
        "async def dangerous_task(ctx: Context) -> str:\n",
        "  \"\"\"A dangerous task that requires human confirmation.\"\"\"\n",
        "\n",
        "  # emit an event to the external stream to be captured\n",
        "  ctx.write_event_to_stream(\n",
        "      InputRequiredEvent(\n",
        "          prefix=\"Are you sure you want to proceed?\",\n",
        "          user_name=\"Ellie\"\n",
        "      )\n",
        "  )\n",
        "\n",
        "  # wait untill a HumanResposeEvent\n",
        "  response = await ctx.wait_for_event(\n",
        "      HumanResponseEvent, requirements={\"user_name\": \"Ellie\"}\n",
        "  )\n",
        "\n",
        "  # act on the input from the event\n",
        "  if response.response.stri().lower() == \"yes\":\n",
        "    return \"Dangerous task completed successfully.\"\n",
        "  else:\n",
        "    return \"Dangerous task aborted.\""
      ],
      "metadata": {
        "id": "i3j-68SBSRMI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To capture the event, use the streaming interface with `InputRequiredEvent` to capture a response from the user and sent it back using `send_event` method"
      ],
      "metadata": {
        "id": "l0IY9k_b33eZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workflow = AgentWorkflow.from_tools_or_functions(\n",
        "      [dangerous_task],\n",
        "      llm=llm,\n",
        "      system_prompt=\"You are a helpful assistant that can perform a dangerous task.\"\n",
        "  )\n",
        "\n",
        "handler = workflow.run(user_msg=\"I want to proceed with the dangerous task.\")\n",
        "\n",
        "async for event in handler.stream_events():\n",
        "  # capture InputRequiredEvent\n",
        "  if isinstance(event, InputRequiredEvent):\n",
        "    # capture keyboard input\n",
        "    response = input(event.prefix)\n",
        "    # send our response back\n",
        "    handler.ctx.send_event(\n",
        "        HumanResponseEvent(\n",
        "            response=response,\n",
        "            user_name=event.user_name\n",
        "        )\n",
        "      )\n",
        "\n",
        "response = await handler\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocbgbyNZ4xh5",
        "outputId": "1b8faf33-a4a4-410a-834e-2642ece0120b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Are you sure you want to proceed?Yes\n",
            "The dangerous task has been confirmed and is now being executed. Please proceed with the necessary precautions.\n"
          ]
        }
      ]
    }
  ]
}